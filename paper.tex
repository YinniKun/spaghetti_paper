
% Title page
\date{}
\title{SPAGHETTI Optimizes the Quantitative Analysis of \textit{In Vitro} Phase Contrast Miscroscopy through a Generative Deep Learning Model}
\author[1-3]{Zhi Fei (Richard) Dong}
\author[1-3]{Chris McIntosh}
\author[1-3]{Gregory W. Schwartz}
\affil[1]{Princess Margaret Cancer Centre, University Health Network, Toronto, ON M5G 1L7, Canada}
\affil[2]{Department of Medical Biophysics, University of Toronto, Toronto, ON M5G 1L7, Canada}
\affil[3]{Vector Institute, Toronto, ON M5G 1M1, Canada}
\affil[
]{\href{mailto:gregory.schwartz@uhn.ca}{email:gregory.schwartz@uhn.ca}}
%\affil[*]{These authors contributed equally to this work}

\maketitle

\begin{refsegment}

\begin{abstract}
  \noindent \textbf{Motivation:} This is important.

  \noindent \textbf{Results:} We present this.

  \noindent \textbf{Availability:} \todo{transfer and clean up code + add to SchwartzLab Github} is freely available at \url{}.
\end{abstract}

\section*{Introduction}

Phase contrast microscopy (PCM) is an old yet widely used techniques in \textit{in vitro} cell biology research. Comparing to other non-stained methods such as bright-field microscopy, PCM provides a more detailed and contrasted image of the cells by converting the phase shift of the light passing through the cells into brightness changes \autocite{pcm_background}. Its ability to visualize the internal structures of the cells without the need of staining also makes it a popular choice for quick live-cell imaging and temporal cell culture monitoring as long time monitoring of fluorescence stains are known to cause phototoxicity in the cells \autocite{phototoxicity}. However, despite its popularity in cell state monitoring, PCM images are rarely used in analysis routines by biologists. This is because even though they can provide a lot of information about the cells, few attempts have been made to extract this information quantitatively, making biologists relying on assays measurments and readings from stain intensities to investigate their properties of interests. This not only adds additional steps and costs to the experiments, but also limits the amount of information that can be extracted from the images as assays and stain intensities can only measure very limited number of properties at once.

Similar problems have been faced by pathologists in the analysis of histopathology images before. Hematoxylin and eosin (H\&E) staining is the most common staining method used in pathology to visualize the tissue structures and cell morphology and can provide a lot of rich information about the tissue disease state. However, for a long time, the information that can be extracted from H\&E images is limited to the human eye, and the analysis of H\&E images is often qualitative and subjective \autocite{he_stain}. Thanks to the development of deep learning, various approaches have been proposed to develop foundational feature extractors to extract the features from H\&E images and use them for downstream tasks such as making diagnosis and survival prediction. Recently, some attempts have even been made to make inferences of the gene expressions from H\&E images, suggesting a vast amount of information can be extracted by deep learning by looking at the cellular morphologies to make H\&E analysis more quantitative. \todo{add citations}


Order of the whole paper: Abstract, Introduction, Results, Discussion, Methods,
Data Availability, Code Availability, Acknowledgments, Funding, Author
Contributions, Competing Interests, Supplementary Information

There is where the intro
goes~\autocite{schwartzTooManyCellsIdentifiesVisualizes2020}. This includes a
short literature review highlighting the gaps that your paper fills in. Ends
with a summary of the paper.

\section*{Results}

\section*{Directory info}

For the code, we want \texttt{main.tex} to have everything (paper and rebuttal).
We comment out rebuttal for initial submission. Paper text is in
\texttt{paper.tex} and rebuttal text is in \texttt{rebuttal/rebuttal.tex}.
Figures go in the \texttt{figures} folder (both \texttt{pdf} and \texttt{svg}),
rebuttal figures go in \texttt{rebuttal/figures}.

To build the document, run \texttt{latexmk -lualatex main.pdf}.

\subsection*{Figure references}

Let me tell you about this amazing result
(\Cref{fig:amazing,fig:astounding,fig:incredible,fig:anotherAmazing,fig:anotherWow,sfig:okay,sfig:maybe,sfig:alright,tab:description,stab:goodStuff}).

\subsection*{Revisions}

\qc{fixed}{\highLight{We also did stuff here for the revision that will appear
    in the rebuttal. Notice how references to figures are correct
    (\Cref{fig:astounding}).}}

\subsection*{Figure style}

\begin{enumerate}
\item \textit{Make sure your figure style is consistent throughout!}
\item We usually use 1px line width for plots (or 0.2mm, but Scientific Inkscape
  defaults to 1px so it's easy to just use that).
\item Nothing smaller than 8pt font, if you can't fit the text with that size in
  the plot then make the plot bigger. For panel labels (a, b, etc.) use 12pt
  bold lowercase.
\item Consistent color scheme throughout. Don't mix color schemes. We usually
  use Set1 from ColorBrewer.
\item Use alignment, don't eyeball centering. Make sure plots and panel labels
  are aligned as much as possible with each other.
\item Don't use gridlines unless they are absolutely necessary
\item Don't squish plots. It looks bad. If you have trouble resizing with too
  much text, use \url{https://github.com/burghoff/Scientific-Inkscape}.
  Scientific Inkscape has a \textit{lot} of very useful utilities for editing
  scientific figures.
\end{enumerate}

\subsection*{Language}

Importantly, in this section you should not write dryly. You must tell a story
with a high-level view of the methods, more detail goes into the methods
section. How specifically should this be written? Read other papers to see how
they do it! Check out \textit{Nature Methods} papers. For example from us
(figures don't match, just using as an example):

\begin{quote}
In order to understand how survival programs could be affected by the duration
of treatment, we sought to characterize the unique expression profiles among
persister-cell populations. We identified differentially expressed genes between
control and persister cells of each cell line separately and aggregated the
complete list of genes using rank product analysis
\autocite{breitlingFEBSLetters2004}. The batch functionality of TMCI allowed us
to efficiently visualize the distribution of top-ranking most differentially
expressed genes across the entire data-set collection. From these
visualizations, we identified \textit{ID2} as one of the most highly upregulated
genes across long-term treated cells in comparison to controls (rank product: 4,
permutation test: \(p < \SI{2.22e-16}{}\)), but not among the short-term treated
cells (rank product: 380, permutation test: \(p < \SI{2.22e-16}{}\))
(\Cref{fig:astounding,stab:goodStuff,sfig:okay}). Comparison of
  \textit{ID2} expression between each control and corresponding treatment arm
  showed a significant increase of \(\log_2\) fold change values for all cell
  lines (Mann-Whitney \(U\) test: \(p < \SI{0.05}{}\)), regardless of treatment
  duration, with the exception of short-term treated DND-41 cells
  (\Cref{fig:amazing} and \Cref{stab:goodStuff}). \textit{ID2} is known to play
  a role in tumorigenesis as a key regulator of cell-cycle progression and
  overexpression of \textit{ID2} in cell-line experiments modulates
  proliferative capacity and cell invasiveness \autocite{itahana_role_2003,
    stighall_high_2005}. Differential \textit{ID2} expression in our analysis
  suggests varying proliferative activity between treatment durations.
\end{quote}

\section*{Discussion}

This section is for summarizing the paper, offering more speculative
interpretations, and suggesting future work and impact.

\section*{Methods}
\subsection*{SPAGHETTI Model Architecture}
SPAGHETTI is built upon the orginial cyleGAN architecture for image translation \autocite{cyclegan2017}. Given a set of images from PCM ($P$) and H\&E ($H$) \todo{ensure PCM and HE are defined previously}, the goal of the model is to learn two mappings ($G_{PH}: P \rightarrow H$ and $G_{HP}: H \rightarrow P$) between a set of training images of PCM (denoted as $\{p_{i}\}^{N}_{i=1}$, where $p_{i} \epsilon P$) and H\&E (denoted as $\{h_{j}\}^{M}_{j=1}$, where $h_{j} \epsilon H$). Note that this does not require an equal number of images from both domains (ie: $N$ does not need to equal to $M$), nor require the images to be paried (ie: $p_{i}$ and $h_{j}$ are the same field of view of the same cells). To accomplished this, we empoly two adversarial discriminators, ($D_{P}$ and $D_{H}$), to distinguish between real and fake images generated by $G_{PH}$ and $G_{HP}$. 

SPAGHETTI utilizes the three different loss functions implemented in the original cycleGAN paper, which are the adversarial loss ($\mathcal{L}_{GAN}$), the cycle-consistency loss ($\mathcal{L}_{cyc}$), and the identity loss ($\mathcal{L}_{idt}$). The adversarial loss is used to train the generators to generate realistic images, while the cycle-consistency loss is used to ensure that the images generated after two generators are consistent with the input images. The identity loss is used to ensure that the generators do not alter the input images if the images are passed through the generator from the same domain. Those three loss functions are defined as follows:

\begin{equation}
  \begin{aligned}
  \mathcal{L}_{GAN}(G_{PH}, D_{H}, P, H) = \mathbb{E}_{h \epsilon H}[\log D_{H}(h)] + \mathbb{E}_{p \epsilon P}[\log(1 - D_{H}(G_{PH}(p)))] \\
  \mathcal{L}_{GAN}(G_{HP}, D_{P}, H, P) = \mathbb{E}_{p \epsilon P}[\log D_{P}(p)] + \mathbb{E}_{h \epsilon H}[\log(1 - D_{P}(G_{HP}(h)))] \\
  \mathcal{L}_{cyc}(G_{PH}, G_{HP}) = \mathbb{E}_{p \epsilon P}[\|G_{HP}(G_{PH}(p)) - p\|_{1}] + \mathbb{E}_{h \epsilon H}[\|G_{PH}(G_{HP}(h)) - h\|_{1}] \\
  \mathcal{L}_{idt}(G_{PH}, G_{HP}) = \mathbb{E}_{p \epsilon P}[\|G_{HP}(p) - p\|_{1}] + \mathbb{E}_{h \epsilon H}[\|G_{PH}(h) - h\|_{1}] \\
  \end{aligned}
\end{equation}


In addition, to solve the issues of hallusination, which is a major concern in medical image translation \autocite{hallucination}, we introduced a new loss function called the SSIM loss ($\mathcal{L}_{SSIM}$) \todo{ensure SSIM is defined properly before}. The SSIM loss is used to penalize the generator if the generated images are not consistent with the input images by introducing or neglecting additional features. The SSIM loss is calculated from the SSIM value, which uses a sliding window approach and the SSIM value is calculated for each window and averaged across the entire image. The SSIM value is a measure of the similarity between two images, which ranges from -1 to 1, where 1 indicates that the two images are identical \autocite{ssim}. It is calculated based on the luminance, contrast, and structure of the images $x$ and $y$ as follows:

\begin{equation}
  \begin{aligned}
  SSIM(x, y) = \frac{(2\mu_{x}\mu_{y} + c_{1})(2\sigma_{xy} + c_{2})}{(\mu_{x}^{2} + \mu_{y}^{2} + c_{1})(\sigma_{x}^{2} + \sigma_{y}^{2} + c_{2})}
  \end{aligned}
\end{equation}

where $\mu_{x}$ and $\mu_{y}$ are the average of $x$ and $y$, $\sigma_{x}^{2}$ and $\sigma_{y}^{2}$ are the variance of $x$ and $y$, $\sigma_{xy}$ is the covariance of $x$ and $y$, and $c_{1}$ and $c_{2}$ are constants to stabilize the division with weak denominator. The SSIM loss is then defined as follows:

\begin{equation}
  \begin{aligned}
  \mathcal{L}_{SSIM}(G_{PH}, G_{HP}) = \frac{1}{N} \sum_{i=1}^{N} \frac{(1 - SSIM(G_{PH}(p_{i}), p_{i}))}{2} + \frac{1}{M} \sum_{j=1}^{M} \frac{(1 - SSIM(G_{HP}(h_{j}), h_{j}))}{2}
  \end{aligned}
\end{equation}

The overall loss is a combination of all the above four losses, which is defined as follows:

\begin{equation}
  \begin{aligned}
    \mathcal{L}(G_{PH}, G_{HP}, D_{P}, D_{H}) = & \mathcal{L}_{GAN}(G_{PH}, D_{H}, P, H) \\
    & + \mathcal{L}_{GAN}(G_{HP}, D_{P}, H, P) \\
    & + \lambda_{cyc}\mathcal{L}_{cyc}(G_{PH}, G_{HP}) \\
    & + \lambda_{idt}\mathcal{L}_{idt}(G_{PH}, G_{HP}) \\
    & + \lambda_{SSIM}\mathcal{L}_{SSIM}(G_{PH}, G_{HP})
  \end{aligned}
\end{equation}

where $\lambda_{cyc}$, $\lambda_{idt}$, and $\lambda_{SSIM}$ are the hyperparameters that control the relative importance of the cycle-consistency, identity, and SSIM loss terms respectively. The overall objective function is optimized by solving the following min-max problem, where the generators are optimized to minimize the loss function, while the discriminators are optimized to maximize the loss function:

\begin{equation}
  \begin{aligned}
    \min_{G_{PH}, G_{HP}} \max_{D_{P}, D_{H}} \mathcal{L}(G_{PH}, G_{HP}, D_{P}, D_{H})
  \end{aligned}
\end{equation}

\subsection*{Training Details}
We trained SPAGHETTI and an original cycleGAN architecture using the LIVECell dataset \autocite{LIVECell} for PCM (N = 5326) and the PanNuke dataset \autocite{pannuke} for H\&E (M = 7904). The RGB images of PCM were cropped randomly to a quarter of its original size while the H\&E images were kept as is, and all images were resized to 256x256 pixels and converted to the range of [0, 1]. They were then normalized to a mean of 0.5 and a standard deviation of 0.5. The generators were implemented using a ResNet architecture \autocite{resnet} with 9 residual blocks. We split the dataset to 80\% training and 20\% testing, before using a batch size of 16 and the AdamW optimizer \autocite{adamw} with a learning rate of 0.001 and a weight decay of 0.01 for training. The hyperparameters $\lambda_{cyc}$ and $\lambda_{idt}$ were set to 10.0 and 5.0 for both SPAGHETTI and the original cycleGAN architecture, and $\lambda_{SSIM}$ was set to 3.0 for SPAGHETTI. Both models were trained for 100 epochs on 8 NVIDIA T4 Tensor Core GPUs on Vector Institute's Vaughan HPC. After the training was completed, we utilized the PCM to H\&E generator ($G_{PH}$) as the generators for SPAGHETTI and the original cycleGAN to generate H\&E-like images from PCM images. 

\subsection*{LIVECell Feature Attention Visualization and Quantification}
We visualized the attention map of the \textit{CLS} token, which is utilized as the feature of image \autocite{vit}, from the last transformer layer of Phikon-v2, an open source foundational feature extractor designed for H\&E images \autocite{phikonv2}, on the 20\% testing portion of the LIVECell dataset. We reshaped the \textit{CLS} token to be the shape of 16x16 pixels, before extrapolating it back bicubicly to the original image size of 256x256 pixels. We then computed the attention map by averaging the attention weights across all heads and normalizing the attention weights to the range of [0, 1]. We finally overlaid the attention map on the original image to visualize the feature attention by utilizing a plasma colour map. 

To quantify whether the transformation allows the model to focus better on the cell populations of the images, we took the top 60\% of the attention score as the predicted cell masks and calculated the Dice score with the ground truth cell masks. The Dice score is defined as follows by comparing the ground truth cell masks ($A$) and the predicted cell masks ($B$):

\begin{equation} \label{eq:dice}
  \begin{aligned}
    Dice = \frac{2|A \cap B|}{|A| + |B|}
  \end{aligned}
\end{equation}

We repeated the above process for using SPAGHETTI as the pre-processing step, using the original cycleGAN architecture as the pre-processing step, and using the original PCM images before feeding the images into Phikon-v2. We performed two-tailed paired t-tests to compare the Dice scores between the different pre-processing steps.

\subsection*{LIVECell Cell Segmentation}
We performed cell segmnetaions using the two popular existing cell segmentors CellPose \autocite{cellpose} and StarDist \autocite{stardist} on the 20\% testing portion of the LIVECell dataset. In particular, we used the tissuenet\_cp3 model for CellPose and the 2D\_versatile\_he model for StarDist. We reported the segmentation results using the Dice coefficient defined in equation (\ref{eq:dice}). We repeated the above process for using SPAGHETTI as the pre-processing step, using the original cycleGAN architecture as the pre-processing step, and using the original PCM images before feeding the images into the cell segmentors. As a control for CellPose, we also pass the original PCM images through the cyto3 model of CellPose. We performed two-tailed paired t-tests to compare the Dice scores between the different pre-processing steps.

\subsection*{LIVECell Cell Type Classification}
We performed cell type classification on the 20\% testing LIVECell dataset by training a random forest classifer with 100 trees and a maximum depth of 2. We utilized both Phikon-v2 and H-Optimus-0 \autocite{hoptimus0}, two state-of-the-art foundational models for H\&E slides, as feature extractors, as well as a ResNet model pre-trained on ImageNet as the baseline feature extractor. The task was to utilize the features extracted from the feature extractors to classify the image into one of the eight cell types annotated by the LIVECell dataset. We reported the AUC values from the ROC plots and their standard deviations of the random forest classifiers using a 10-fold cross-validation approach. We repeated the above process for using SPAGHETTI as the pre-processing step, using the original cycleGAN architecture as the pre-processing step, and using the original PCM images before feeding the images into the feature extractors. We performed two-tailed paired t-tests to compare the classification accuracies between the different pre-processing steps. \todo{check if the stats test is necessary}

\subsection*{C2C12 Cell Media Condition Classification}
To validate the performance of SPAGHETTI outside of the training dataset, we performed cell media condition classification on the C2C12 PCM dataset \autocite{c2c12}. The C2C12 dataset consists of PCM images of C2C12 cells under different culture media conditions, which are: control (no growth factors), fibroblast growth factor 2, bone morphogenetic protein 2, and a combination of fibroblast growth factor 2 and bone morphogenetic protein 2. Similar to classifying cell types for LIVECell, we utilized Phikon-v2 and H-Optimus-0 as our H\&E feature extrators and pre-trained ResNet as our baseline feature extractor and trained a random forest classifier with 100 trees and a maximum depth of 2 to classify the images from the extracted features. We reported the AUC values from the ROC plots and their standard deviations of the random forest classifiers using a 10-fold cross-validation approach. We repeated the above process for using SPAGHETTI as the pre-processing step, using the original cycleGAN architecture as the pre-processing step, and using the original PCM images before feeding the images into the random forest classifier. We performed two-tailed paired t-tests to compare the classification accuracies between the different pre-processing steps. \todo{check if the stats test is necessary} 

\subsection*{We did stuff}

This is how we did it. We used \SI{15}{\micro\meter} keyboards.

\section*{Data Availability}

We obtained spatial transcriptomic data of human lymph node from
\url{https://www.10xgenomics.com/datasets/human-lymph-node-1-standard-1-0-0},
mouse hypothalamic preoptic region from
\url{https://datadryad.org/stash/dataset/doi:10.5061/dryad.8t8s248}, LUAD from
the Gene Expression Omnibus under accession number GSE189487, and human
colorectal cancer (Visium HD) from
\url{https://www.10xgenomics.com/datasets/visium-hd-cytassist-gene-expression-libraries-of-human-crc}.

\section*{Code availability}

\todo{XXXXXX} is available at
\url{https://github.com/schwartzlab-methods/XXXXXXXX} or as a Singularity image
at \url{https://cloud.sylabs.io/library/you/collection/XXXXXXXXX.sif} with a
tutorial at \url{https://github.com/schwartzlab-methods/XXXXXX#vignette}.
Scripts for generating the figures and plots of the manuscript can be found at
\url{https://github.com/schwartzlab-methods/XXXXXXX_paper_figures}.

\section*{Acknowledgments}

We thank a lot of people.

\section*{Funding}

This work was supported by the University of Toronto Data Sciences Institute
Research Software Development Support Program (G. W. S.), the Canadian Cancer
Society Challenge Grant (grant 707484; G. W. S.), the Natural Sciences and
Engineering Research Council of Canada (grants RGPIN-2023-04713 and
DGECR-2023-00395; G. W. S.), the Social Sciences and Humanities Research Council
(grant NFRFE-2022-00681; G. W. S.), the Canada Research Chairs Program (G. W.
S.), and the Princess Margaret Cancer Foundation (G. W. S.).

\section*{Authors Contributions}

G. W. S. conceived and supervised the project. Y. O. U. did everything else.

\section*{Competing Interests}

G. W. S. conceived and supervised the project. Y. O. U. developed the XXXXXXXX
method, software, and benchmarks. Y. O. U. ran and analyzed benchmarks. Y. O. U.
generated experimental results. Y. O. U. ran and analyzed data. Y. O. U. and G.
W. S. wrote and edited the manuscript. All authors reviewed the manuscript.

\clearpage

\end{refsegment}
\printbibliography[segment=1]

\clearpage

\begin{figure}[htbp]
\centering
\includepdf[fitpaper=true,pages=-]{figures/figure_LUAD_CRC.pdf}
\end{figure}

\clearpage

\begin{figure}[htbp]
  {\phantomsubcaption\label{fig:amazing}}
  {\phantomsubcaption\label{fig:astounding}}
  {\phantomsubcaption\label{fig:incredible}}
  \caption{\panel{sub@fig:amazing,sub@fig:astounding,sub@fig:incredible}, Many cool things.
    \panel{sub@fig:amazing}, This is amazing. \panel{sub@fig:astounding}, This
    is astounding. \panel{sub@fig:incredible}, This is incredible.
  \label{fig:incredibleOne}}
\end{figure}

\clearpage

\begin{figure}[htbp]
\centering
\includepdf[fitpaper=true,pages=-]{figures/figure_LUAD_CRC.pdf}
\end{figure}

\clearpage

\begin{figure}[htbp]
  {\phantomsubcaption\label{fig:anotherAmazing}}
  {\phantomsubcaption\label{fig:anotherWow}}
  \caption{\panel{sub@fig:anotherAmazing}, This is also amazing.
    \panel{sub@fig:anotherWow}, Yes indeed.
  \label{fig:astoundingOne}}
\end{figure}

\clearpage

\begin{table}
\begin{center}
  \caption{\label{tab:description} Great table, isn't it?}
  \rowcolors{2}{white}{lightlightgray}
  \begin{tabular}{ llllc }
    \Xhline{2pt}
    My & great & table & wow & amazing\\
    \hline
    Wow & 2 & 4 & 3 & 2\\
    \Xhline{2pt}
  \end{tabular}
\end{center}
\end{table}

\clearpage

%% Reset numbering
\pagenumbering{arabic}

\section*{Supplementary Information}

\subsection*{Supplementary Notes}

\subsection*{\Cref{snote:great}: Great note}\snote{snote:great}

Really remarkable stuff here.

\qc{otherFixInSupplemental}{\highLight{We also did stuff here for the revision
    that will appear in the rebuttal, but in the supplemental section here, so
    note the \texttt{qps} in the rebuttal.}}

\subsection*{Supplementary Figures}

\clearpage

\begin{sfigure}[htbp]
  \centering
  \includepdf[fitpaper=true,pages=-]{supplementary/figure_LUAD_CRC.pdf}
\end{sfigure}

\clearpage

\begin{sfigure}[htbp]
  \centering
  {\phantomsubcaption\label{sfig:okay}}
  {\phantomsubcaption\label{sfig:maybe}}
  {\phantomsubcaption\label{sfig:alright}}
  \caption{\panel{sub@sfig:okay,sub@sfig:maybe}, This describes so many cool
    things. \panel{sub@sfig:okay}, This is okay. \panel{sub@sfig:maybe}, This is
    meh. \panel{sub@sfig:alright}, This is alright.}
  \label{sfig:itsokay}
\end{sfigure}

\clearpage

\section*{Supplementary Tables}

\captionof{stab}{\label{stab:goodStuff} Here is some info.}
